# -*- coding: utf-8 -*-
"""ObjectDetection with FasterR-CNN(module Detectron2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PHQtG4mB55vkk54cNQU0Y36HH6-6RzCp
"""

# install dependencies: 
!pip install -U torch==1.5 torchvision==0.6 -f https://download.pytorch.org/whl/cu101/torch_stable.html
!pip install cython pyyaml==5.1
!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'

import torch, torchvision
print(torch.__version__, torch.cuda.is_available())
!gcc --version

import torch, torchvision

torch.cuda.is_available()

# install detectron2:
!pip install detectron2==0.1.3 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.5/index.html

# You may need to restart your runtime prior to this, to let your installation take effect
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

# import some common libraries
import numpy as np
import cv2
import random
from google.colab.patches import cv2_imshow

# import some common detectron2 utilities
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog

!wget http://images.cocodataset.org/zips/train2017.zip
!wget http://images.cocodataset.org/zips/val2017.zip
#!wget http://images.cocodataset.org/zips/test2017.zip
!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip
!wget http://images.cocodataset.org/annotations/image_info_test2017.zip

#!unzip "train2017.zip" 
!unzip "val2017.zip"
#!unzip "test2017.zip"
!unzip "annotations_trainval2017.zip"
!unzip "image_info_test2017.zip"

!unzip "train2017.zip"

import json
from detectron2.structures import BoxMode
def get_board_dicts(imgdir):
    json_file = imgdir+"/instances_train2017.json" #Fetch the json file
    with open(json_file) as f:
        dataset_dicts = json.load(f)
    for i in dataset_dicts:
        filename = i["file_name"] 
        i["file_name"] = imgdir+"/"+filename 
        for j in i["annotations"]:
            j["bbox_mode"] = BoxMode.XYWH_ABS #Setting the required Box Mode
            j["category_id"] = int(j["category_id"])
    return dataset_dicts
from detectron2.data import DatasetCatalog, MetadataCatalog
#Registering the Dataset
for d in ["train", "val"]:
    DatasetCatalog.register("boardetect_" + d, lambda d=d: get_board_dicts("Text_Detection_Dataset_COCO_Format/" + d))
    MetadataCatalog.get("boardetect_" + d).set(thing_classes=list)
board_metadata = MetadataCatalog.get("boardetect_train")

import json
from detectron2.structures import BoxMode

imgdir = "/content/train2017"
json_file = imgdir+"/instances_train2017.json"

with open(json_file) as f:
        dataset_dicts = json.load(f)

for jsn_val in dataset_dicts.values():
    print(jsn_val)

for jsn_key in dataset_dicts:
    print(jsn_key)

for i in dataset_dicts:
        filename = i["file_name"] 
        print(filename)

import json
from detectron2.structures import BoxMode

from detectron2.data.datasets import register_coco_instances
register_coco_instances("coco_train", {}, "/content/annotations/instances_val2017.json", "/content/val2017")

from detectron2.data import DatasetCatalog, MetadataCatalog

dataset_dicts = DatasetCatalog.get("coco_train")

coco_metadata_train = MetadataCatalog.get("coco_train")

coco_metadata_train







!cp /content/annotations/instances_train2017.json /content/train2017/

list=[
 'person',
 'bicycle',
 'car',
 'motorcycle',
 'airplane',
 'bus',
 'train',
 'truck',
  'boat',
  'traffic light',
  'fire hydrant',
  'stop sign',
  'parking meter',
  'bench',
  'bird',
  'cat',
  'dog',
  'horse',
  'sheep',
  'cow',
  'elephant',
  'bear',
  'zebra',
  'giraffe',
  'backpack',
  'umbrella',
  'handbag',
  'tie',
  'suitcase',
  'frisbee',
  'skis',
  'snowboard',
  'sports ball',
  'kite',
  'baseball bat',
  'baseball glove',
  'skateboard',
  'surfboard',
  'tennis racket',
  'bottle',
  'wine glass',
  'cup',
  'fork',
  'knife',
  'spoon',
  'bowl',
  'banana',
  'apple',
  'sandwich',
  'orange',
 'broccoli',
 'carrot',
 'hot dog',
 'pizza',
 'donut',
 'cake',
 'chair',
 'couch',
 'potted plant',
 'bed',
 'dining table',
 'toilet',
 'tv',
 'laptop',
 'mouse',
 'remote',
 'keyboard',
 'cell phone',
 'microwave',
 'oven',
 'toaster',
 'sink',
 'refrigerator',
 'book',
 'clock',
 'vase',
 'scissors',
 'teddy bear',
 'hair drier',
 'toothbrush']

import random
from detectron2.utils.visualizer import Visualizer

for d in random.sample(dataset_dicts, 3):
    img = cv2.imread(d["file_name"])
    visualizer = Visualizer(img[:, :, ::-1], metadata=coco_metadata_train, scale=0.5)
    vis = visualizer.draw_dataset_dict(d)
    cv2_imshow(vis.get_image()[:, :, ::-1])

for d in ["train", "val"]:
    DatasetCatalog.register("boardetect_" + d, lambda d=d: get_board_dicts("Text_Detection_Dataset_COCO_Format/" + d))
    MetadataCatalog.get("boardetect_" + d).set(thing_classes=["HINDI","ENGLISH","OTHER"])
board_metadata = MetadataCatalog.get("boardetect_train")

from detectron2.engine import DefaultTrainer
from detectron2.config import get_cfg
import os
cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml")) #Get the basic model configuration from the model zoo 
#Passing the Train and Validation sets
cfg.DATASETS.TRAIN = ("coco_train",)
cfg.DATASETS.TEST = ()
# Number of data loading threads
cfg.DATALOADER.NUM_WORKERS = 4
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml")  # Let training initialize from model zoo
# Number of images per batch across all machines.
cfg.SOLVER.IMS_PER_BATCH = 4
cfg.SOLVER.BASE_LR = 0.0125  # pick a good LearningRate
cfg.SOLVER.MAX_ITER = 1500  #No. of iterations   
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256  
cfg.MODEL.ROI_HEADS.NUM_CLASS = 80 # No. of classes = [HINDI, ENGLISH, OTHER]
cfg.TEST.EVAL_PERIOD = 500 # No. of iterations after which the Validation Set is evaluated. 
os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
trainer = CocoTrainer(cfg) 
trainer.resume_or_load(resume=False)
trainer.train()

register_coco_instances("coco2_test", {}, "/content/annotations/instances_val2017.json", "/content/val2017")

dataset_dicts_val = DatasetCatalog.get("coco2_test")

coco_metadata_val

= Datacoco2_test", {}, "/content/annotations/instances_val2017.json", "/content/val2017")setCatalog.get("coco2_test")

import random
from detectron2.utils.visualizer import Visualizer

for d in random.sample(dataset_dicts_val, 3):
    img = cv2.imread(d["file_name"])
    visualizer = Visualizer(img[:, :, ::-1], metadata=coco_metadata_val, scale=0.5)
    vis = visualizer.draw_dataset_dict(d)
    cv2_imshow(vis.get_image()[:, :, ::-1])

from detectron2.engine import DefaultTrainer
from detectron2.config import get_cfg
import os
cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml")) #Get the basic model configuration from the model zoo 
#Passing the Train and Validation sets
cfg.DATASETS.TRAIN = ("coco_train",)
cfg.DATASETS.TEST = ()
# Number of data loading threads
cfg.DATALOADER.NUM_WORKERS = 4
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml")  # Let training initialize from model zoo
# Number of images per batch across all machines.
cfg.SOLVER.IMS_PER_BATCH = 4
cfg.SOLVER.BASE_LR = 0.0125  # pick a good LearningRate
cfg.SOLVER.MAX_ITER = 1500  #No. of iterations   
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256  
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 80 # No. of classes = [HINDI, ENGLISH, OTHER]
cfg.TEST.EVAL_PERIOD = 500 # No. of iterations after which the Validation Set is evaluated. 
os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
trainer = DefaultTrainer(cfg) 
trainer.resume_or_load(resume=False)
trainer.train()

cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set the testing threshold for this model
cfg.DATASETS.TEST = ("coco_train", )
predictor = DefaultPredictor(cfg)

predictor

from detectron2.utils.visualizer import ColorMode

for d in random.sample(dataset_dicts, 3):    
    im = cv2.imread(d["file_name"])
    outputs = predictor(im)
    v = Visualizer(im[:, :, ::-1],
                   metadata=coco_metadata_train, 
                   scale=0.8, 
                   instance_mode=ColorMode.IMAGE   # remove the colors of unsegmented pixels
    )
    v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    cv2_imshow(v.get_image()[:, :, ::-1])

from detectron2.utils.visualizer import ColorMode

#Use the final weights generated after successful training for inference  
cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")

cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8  # set the testing threshold for this model
#Pass the validation dataset
cfg.DATASETS.TEST = ("boardetect_val", )

predictor = DefaultPredictor(cfg)

dataset_dicts = get_board_dicts("/content/gazou")
for d in random.sample(dataset_dicts, 3):    
    im = cv2.imread(d["file_name"])
    outputs = predictor(im)
    v = Visualizer(im[:, :, ::-1],
                   metadata=coco_metadata_train, 
                   scale=0.8,
                   instance_mode=ColorMode.IMAGE   
    )
    v = v.draw_instance_predictions(outputs["instances"].to("cpu")) #Passing the predictions to CPU from the GPU
    cv2_imshow(v.get_image()[:, :, ::-1])

im = cv2.imread("/content/gazou/000000215230.jpg")
outputs = predictor(im)
v = Visualizer(im[:, :, ::-1],
                   metadata=coco_metadata_train, 
                   scale=0.8,
                   instance_mode=ColorMode.IMAGE   
    )
v = v.draw_instance_predictions(outputs["instances"].to("cpu")) #Passing the predictions to CPU from the GPU
cv2_imshow(v.get_image()[:, :, ::-1])

#/content/gazou/apple.jpeg


im = cv2.imread("/content/gazou/apple.jpeg")
outputs = predictor(im)
v = Visualizer(im[:, :, ::-1],
                   metadata=coco_metadata_train, 
                   scale=0.8,
                   instance_mode=ColorMode.IMAGE   
    )
v = v.draw_instance_predictions(outputs["instances"].to("cpu")) #Passing the predictions to CPU from the GPU
cv2_imshow(v.get_image()[:, :, ::-1])

#/content/gazou/apple.jpeg


im = cv2.imread("/content/gazou/cat.jpeg")
outputs = predictor(im)
v = Visualizer(im[:, :, ::-1],
                   metadata=coco_metadata_train, 
                   scale=0.8,
                   instance_mode=ColorMode.IMAGE   
    )
v = v.draw_instance_predictions(outputs["instances"].to("cpu")) #Passing the predictions to CPU from the GPU
cv2_imshow(v.get_image()[:, :, ::-1])

#/content/gazou/motorcycle(元画像).jpg
im = cv2.imread("/content/gazou/motorcycle(元画像).jpg")
outputs = predictor(im)
v = Visualizer(im[:, :, ::-1],
                   metadata=coco_metadata_train, 
                   scale=0.8,
                   instance_mode=ColorMode.IMAGE   
    )
v = v.draw_instance_predictions(outputs["instances"].to("cpu")) #Passing the predictions to CPU from the GPU
cv2_imshow(v.get_image()[:, :, ::-1])

# 推論対象のテスト画像一覧(List of test images to be inferred)
import glob
import copy
import cv2
from google.colab.patches import cv2_imshow

test_data_dir = '/content/train2017/'
testfile_list = sorted(glob.glob(test_data_dir + '/*.jpg'))

len(testfile_list)

randomelist =[random.randint(0, 40330) for i in range(5)]
testfile_listA = [testfile_list[randomelist[i]] for i in range(len(randomelist))]
for i in range(len(testfile_listA)):
  im = cv2.imread(testfile_listA[i])
  outputs = predictor(im)
  v = Visualizer(im[:, :, ::-1],
                   metadata=coco_metadata_train, 
                   scale=0.8,
                   instance_mode=ColorMode.IMAGE   
    )
  v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
  cv2_imshow(v.get_image()[:, :, ::-1])

randomelist =[random.randint(0, 40330) for i in range(5)]
testfile_listA = [testfile_list[randomelist[i]] for i in range(len(randomelist))]
for i in range(len(testfile_listA)):
  im = cv2.imread(testfile_listA[i])
  outputs = predictor(im)
  v = Visualizer(im[:, :, ::-1],
                   metadata=coco_metadata_train, 
                   scale=0.8,
                   instance_mode=ColorMode.IMAGE   
    )
  v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
  cv2_imshow(v.get_image()[:, :, ::-1])

randomelist =[random.randint(0, 40330) for i in range(5)]
testfile_listA = [testfile_list[randomelist[i]] for i in range(len(randomelist))]
for i in range(len(testfile_listA)):
  im = cv2.imread(testfile_listA[i])
  outputs = predictor(im)
  v = Visualizer(im[:, :, ::-1],
                   metadata=coco_metadata_train, 
                   scale=0.8,
                   instance_mode=ColorMode.IMAGE   
    )
  v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
  cv2_imshow(v.get_image()[:, :, ::-1])

randomelist =[random.randint(0, 40330) for i in range(5)]
testfile_listA = [testfile_list[randomelist[i]] for i in range(len(randomelist))]
for i in range(len(testfile_listA)):
  im = cv2.imread(testfile_listA[i])
  outputs = predictor(im)
  v = Visualizer(im[:, :, ::-1],
                   metadata=coco_metadata_train, 
                   scale=0.8,
                   instance_mode=ColorMode.IMAGE   
    )
  v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
  cv2_imshow(v.get_image()[:, :, ::-1])





#/content/gazou/motorcycle(元画像).jpg
im = cv2.imread("/content/gazou/motorcycle(元画像).jpg")
outputs = predictor(im)
v = Visualizer(im[:, :, ::-1],
                   metadata=coco_metadata_train, 
                   scale=0.8,
                   instance_mode=ColorMode.IMAGE   
    )
v = v.draw_instance_predictions(outputs["instances"].to("cpu")) #Passing the predictions to CPU from the GPU
cv2_imshow(v.get_image()[:, :, ::-1])

from detectron2.utils.visualizer import ColorMode

for d in random.sample(dataset_dicts, 3):    
    im = cv2.imread(d["file_name"])
    outputs = predictor(im)
    v = Visualizer(im[:, :, ::-1],
                   metadata=coco_metadata_train, 
                   scale=0.8, 
                   instance_mode=ColorMode.IMAGE   # remove the colors of unsegmented pixels
    )
    v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    cv2_imshow(v.get_image()[:, :, ::-1])



from detectron2.utils.visualizer import ColorMode

#Use the final weights generated after successful training for inference  
cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")

cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8  # set the testing threshold for this model
#Pass the validation dataset
cfg.DATASETS.TEST = ("boardetect_val", )

predictor = DefaultPredictor(cfg)

dataset_dicts = get_board_dicts("/content/gazou/")
for d in random.sample(dataset_dicts, 3):    
    im = cv2.imread(d["file_name"])
    outputs = predictor(im)
    v = Visualizer(im[:, :, ::-1],
                   metadata=board_metadata, 
                   scale=0.8,
                   instance_mode=ColorMode.IMAGE   
    )
    v = v.draw_instance_predictions(outputs["instances"].to("cpu")) #Passing the predictions to CPU from the GPU
    cv2_imshow(v.get_image()[:, :, ::-1])

